{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# load tweets\n",
    "css_authors = pd.read_csv(\"path/to/tweet/file.tsv\", sep=\"\\t\")\n",
    "css_authors[\"dict\"] = css_authors[\"json_str\"].apply(lambda x: json.loads(x))\n",
    "\n",
    "# load fake account IDs\n",
    "fake_accounts = pd.read_csv(\"path/to/status_file.csv\")\n",
    "\n",
    "# select English tweets from accounts that are still active/alive\n",
    "alive_ids = list(fake_accounts[fake_accounts[\"status\"] == \"alive\"][\"Unnamed: 0\"])\n",
    "alive_ids = [str(x) for x in alive_ids]\n",
    "\n",
    "data = [\n",
    "    x\n",
    "    for x in css_authors[\"dict\"]\n",
    "    if (\n",
    "        datetime.strptime(x[\"data\"][\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").year >= 2023\n",
    "    )\n",
    "    and x[\"data\"][\"lang\"] == \"en\"\n",
    "]\n",
    "data_available_user = [x for x in data if x[\"data\"][\"author_id\"] in alive_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sentence transformer\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda:0\")\n",
    "\n",
    "# extract and pre-process tweet texts\n",
    "tweets = [x[\"data\"][\"text\"] for x in data_available_user]\n",
    "ignore_words = [\"rt\"]\n",
    "tweets_text_unique = [x.lower() for x in tweets]\n",
    "tweets_text_unique = set(tweets_text_unique)\n",
    "tweets_text_unique = [\n",
    "    \" \".join([w for w in t.split() if w not in ignore_words])\n",
    "    for t in tweets_text_unique\n",
    "]\n",
    "corpus_sentences = [x for x in tweets_text_unique]\n",
    "max_corpus_size = len(corpus_sentences)\n",
    "\n",
    "# encode corpus\n",
    "corpus_sentences = corpus_sentences[0:max_corpus_size]\n",
    "print(\"Encode the corpus. This might take a while\")\n",
    "corpus_embeddings = model.encode(\n",
    "    corpus_sentences, batch_size=64, show_progress_bar=True, convert_to_tensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start clustering\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Two parameters to tune:\n",
    "# min_cluster_size: Only consider cluster that have at least 100 elements\n",
    "# threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar\n",
    "# clusters = util.community_detection(corpus_embeddings, min_community_size=8, threshold=0.4)\n",
    "clusters = util.community_detection(\n",
    "    corpus_embeddings, min_community_size=47, threshold=0.6, batch_size=1024\n",
    ")\n",
    "\n",
    "print(\"Clustering done after {:.2f} sec\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print for all clusters the top 3 and bottom 3 elements\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(\"\\nCluster {}, #{} Elements \".format(i + 1, len(cluster)))\n",
    "    for sentence_id in cluster[0:10]:\n",
    "        print(sentence_id)\n",
    "        print(\"\\t\", corpus_sentences[sentence_id])\n",
    "    print(\"\\t\", \"...\")\n",
    "    for sentence_id in cluster[-1:]:\n",
    "        print(\"\\t\", corpus_sentences[sentence_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(\n",
    "        ngram_range=ngram_range, stop_words=\"english\", token_pattern=r\"[^\\s]+\"\n",
    "    ).fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "\n",
    "\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names_out()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {\n",
    "        label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1]\n",
    "        for i, label in enumerate(labels)\n",
    "    }\n",
    "    return top_n_words\n",
    "\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (\n",
    "        df.groupby([\"Topic\"])\n",
    "        .Doc.count()\n",
    "        .reset_index()\n",
    "        .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis=\"columns\")\n",
    "        .sort_values(\"Size\", ascending=False)\n",
    "    )\n",
    "    return topic_sizes\n",
    "\n",
    "\n",
    "def create_representative_table(clusters, top_n_words):\n",
    "    representative_table = pd.DataFrame({\"cluster\": [], \"text\": []})\n",
    "    # Print for all clusters the top 3 and bottom 3 elements\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        for sentence_id in cluster[0:1]:\n",
    "            representative_table = pd.concat(\n",
    "                [\n",
    "                    representative_table,\n",
    "                    pd.DataFrame(\n",
    "                        [\", \".join([x[0] for x in top_n_words[i + 1]][0:8])],\n",
    "                        columns=[\"text\"],\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "    representative_table[\"cluster\"] = range(1, len(clusters) + 1)\n",
    "    return representative_table\n",
    "\n",
    "\n",
    "# determine representative class tokens using TF-IDF\n",
    "label = 1\n",
    "lst = [np.nan] * len(corpus_embeddings)\n",
    "for x in clusters:\n",
    "    for y in x:\n",
    "        lst[y] = label\n",
    "    label = label + 1\n",
    "\n",
    "filter_clust = corpus_embeddings\n",
    "\n",
    "\n",
    "docs_df = pd.DataFrame(corpus_sentences, columns=[\"Doc\"])\n",
    "docs_df[\"Topic\"] = lst\n",
    "docs_df[\"Doc_ID\"] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby([\"Topic\"], as_index=False).agg({\"Doc\": \" \".join})\n",
    "\n",
    "\n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(corpus_sentences))\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df)\n",
    "np.sum(topic_sizes.Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.set_option(\"expand_frame_repr\", False)\n",
    "create_representative_table(clusters, top_n_words)[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create UMAP plot\n",
    "plt.rc(\"axes\", unicode_minus=False)\n",
    "fig, (ax1) = plt.subplots()\n",
    "\n",
    "filter_clust = corpus_embeddings\n",
    "filter_clust = filter_clust.cpu()\n",
    "\n",
    "# Prepare data for UMAP\n",
    "umap_data = umap.UMAP(\n",
    "    n_neighbors=20, n_components=2, min_dist=0.4, metric=\"cosine\"\n",
    ").fit_transform(filter_clust.cpu())\n",
    "result = pd.DataFrame(umap_data, columns=[\"x\", \"y\"])\n",
    "\n",
    "result[\"labels\"] = lst\n",
    "res = result\n",
    "\n",
    "outliers = res.loc[result.labels == -1, :]\n",
    "clustered = res.loc[result.labels != -1, :]\n",
    "ax1.scatter(outliers.x, outliers.y, color=\"#BDBDBD\", s=0.05)\n",
    "ax1.scatter(\n",
    "    clustered.x, clustered.y, c=clustered.labels, s=50, cmap=\"tab20\", marker=\"+\"\n",
    ")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "for i in range(1, len(clusters) + 1):\n",
    "    x = np.median(res[res[\"labels\"] == i][\"x\"])\n",
    "    y = np.median(res[res[\"labels\"] == i][\"y\"])\n",
    "    ax1.annotate(i, (x, y), fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
